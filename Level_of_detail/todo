
TODO AND OTHER:

// Iteration 1:
1. Redo selector class using property maps.
2. For now I use built in conditions inside selection strategy classes. Change it to any custom condition later.
3. Refactor the preprocessor class.
4. Update Level_of_detail_loader.h - currently it is fixed temporary implementation.
5. For LOD I can stick to my own simple data structures. For some packages like 2D structuring I can already mimic CGAL general style. In general, first make it work with simple data structures and then fix API.
6. It is important to gradually increase the complexity of the input data set and see if our package handles it well.
7. It is maybe helpful to work building by building and create an oriented bounding box around each building, which can become the building's boundary in the worst case.
8. Too long walls come from shape detection package if the cluster_epsilon parameter is choosen to be too large.
9. Some walls are missing due to the precision problems when creating segments (projection of a point onto a line).
10. I reject all planes that have an angle more than 10 degrees with respect to the closest vertical plane. This allows to solve some of the missclassification problems.
11. Should I enforce the same normals for the regularized plane?
12. Improve the function for getting corners. Use a function to detect all cycles in the undirected graph obtained in the function create_structured_adjacency().
13. Method for adjacency / corners is not robust when wrong planes are detected by the RANSAC. In this case, it may happen that lines that we intersect to get corners produce points which are far away from the real data set.
14. Maybe better to pass all indices except for building boundaries to the visibility class instead of the whole input container?
15. Add one more label to the input: 1 detected as planar, 0 detected as something else. Then add a selector class to selecte points, which are detected as something else and are facade points that is building boundary. Add these points to the final structured set of points with the label CLUTTER.
16. Actuually my reconstruction code should compute the cost function for each surface S in all graph cut iterations.


// Iteration 2:
1. What about creating new semantic data structures such as "Building", "Tree", "Car" and so on?
2. Can I make an existing edge constrained? What if I have one building inside another one. How can I detect/correct after the graph cut that interior building's walls should be constrained?
3. How to get an edge if I know face handle and its neighbouring face handle?
4. Maybe remove CDT from height fitters?
5. Add a new outliner algorithm that will work with multiple boundaries - for example one exterior boundary and one interior boundary.
6. How to get the n-th face iterator: like std::distance(faces.begin(), n)?
7. How to get indices of the faces, vertices from the iterators when saving them in .ply or any other format?
8. Is it possible to add colors to faces in the Polyhedron_incremental_builder class?
9. How to create a 2-manifold or watertight mesh from the polygonal soup, which is LOD1 now?
10. Add classification labels to Level_of_detail_enum.h.
11. Some buildings turn out to be just a triangle that is why some walls of the buildings have different colours.


// Iteration 3:
1. Classification package saves some points with label -1. So it is not guaranteed that all points are classified? Why?
2. Fix bug or whatever it is in the outliner class such that it does not create any inverted faces as now - it may be a Meshlab bug.
3. Remove lonely triangles in the buildings finder by creating a set of constrained edges between in and out after the final graph cut. It may also help in fixing 2. above.
4. The main problem with real data comes from creating corners. In this case, I have to intersect segments or their supporting lines, which creates many points at infinity.
5. In demo classification, radius label changed its position. Actually sometimes radius label is not saved at all. Sometimes, it is added at the end like in the paris data set, and sometimes after nz as in the p10 data set.
6. When cliking anywhere away from the classification pop-in window, the focus disappears and we have to start all over again.
7. p10 data set has the same point twice after running region growing.
8. Segmentation fault when loading the p10 class_trial.ply and changing points+normals to the next mode in the master/Polyhedron_demo.
9. Outliner algorithm is very sensitive to noise and incorrect input data! Weak link. Second weak link is intersection between structured segments and fitting lines to the input data coming from shape detection.
10. Might be a bug in the graph cut since after applying it in the short pipeline I have many tiny triangles, which are in and next to them other tiny triangles, which are out?
11. Short pipeline is easier to set up, requires fewer parameters, much slower to run and its robustness depends directly on the classification result. It is also senstive to the boundaries (wether to compute them or not in the outliner).


// Iteration 4:
1. Natural neighbour function bugs for some sample points and I cannot handle this execption. The bug is due to the insufficient number of points in Delaunay triangulation or because some of its vertices coincide with sample points.
2. Try affine coordinates instead of natural neighbour coordinates in the visibility module.
3. Maybe better to use sphere neighbour search instead of knn in thinning?
4. Natural neighbours approach is very inefficient because we need to build Delaunay triangulation for input data set with thousands of points.
5. Natural neighbour visibility is not very robust because it depends on random sampling of each triangle.
6. I think the main problem with graph cut for the pipeline without shape detection comes from the fact that all edges are free-form coherent and are favoured more or less the same.
7. Parameters for the graph cut:

	a. m_alpha should always be greater or equal 1, though bigger than 1 does not make any difference - so just use 1.
	b. m_beta should be a big value >= 1, increasing it makes less inside triangles.
	c. m_gamma is not used in the clutter-based pipeline (without shape detection); for the structuring-based pipeline (with shape detection), just use some big value. Making it bigger or smaller does not change the result.

8. Thin triangles are not good for the point based visibility.
9. New updated code works much faster.
10. Is there a better way to get the intersected side of the face in the line walk without traversing all face edges?
11. Where I can find a paper about bands for ray shooting?


// Graph cut:
For clutter-based cdt all edges are free-form coherent - Yes.
Remove m_beta from out - Makes all triangles in!
Use squared length instead of length for edge weights - With squared length the result seems to be cut over the longer edge.
Change free_form_quality function to the fixed value or improve it - It looks ok, if I want the pure shortest path cut, use 1 instead of this function - at least for some data sets with some parameters, it works.
I do max flow between faces = graph nodes (favour big values) or min cut over the finite edges = graph edges (favour small values) - Yes.
Add extra coherence to the free-form coherent edges using for example in and out data - Added tanh adapter to node values.


// Iteration 5:
1. Add after graph cut constrained edges between all in and out. All other constrained edges should be removed. In this case we will get a good LOD0 and LOD1. For LOD2 we save info with previously removed constrained edges and build different types of buildings, which are multilevel. Then we can take each building and work per building, split it into multi levels, add roofs, and so on.
2. Should we really change simple version of my unordered outliner to the pivoting one?
3. Should we really change my simple version of the banding in the ray shooting to the complex one of Pierre?
4. Should we really implement the watertight 2-manifold version of the LOD0 and LOD1?

Points: finish and test visibility; finish and test outliner with lod0-lod1; finish and test all other small tasks - sphere search in thinning, bug in structuring, etc.; test everything and prepare the big Paris data set.


